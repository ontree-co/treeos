{
  "id": "ollama-amd-ai370",
  "name": "Ollama (AMD AI 370)",
  "description": "System service for running LLMs with AMD AI 370-class ROCm support. Manages shared model store.",
  "category_tags": [
    "LLM Inference"
  ],
  "icon": "bi-gpu-card",
  "port": "11434",
  "documentation_url": "https://ollama.ai",
  "is_system_service": true,
  "filename": "docker-compose.yml"
}
