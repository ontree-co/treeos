{
  "id": "ollama-nvidia",
  "name": "Ollama (NVIDIA GPU)",
  "description": "System service for running LLMs with NVIDIA GPU acceleration. Manages shared model store.",
  "category_tags": [
    "LLM Inference"
  ],
  "icon": "bi-gpu-card",
  "port": "11434",
  "documentation_url": "https://ollama.ai",
  "is_system_service": true,
  "filename": "docker-compose.yml"
}
